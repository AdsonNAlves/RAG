{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from pymilvus import connections, utility, FieldSchema, CollectionSchema, Collection, DataType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain,LLMChain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate,PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53c035f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregar variáveis de ambiente do .env\n",
    "load_dotenv()\n",
    "\n",
    "POSTGRES_CONFIG = {\n",
    "    \"host\": os.getenv(\"POSTGRES_HOST\", \"localhost\"),\n",
    "    \"port\": os.getenv(\"POSTGRES_PORT\", \"5432\"),\n",
    "    \"dbname\": os.getenv(\"POSTGRES_DB\", \"app_db\"),\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\", \"admin\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\", \"admin\")\n",
    "}\n",
    "\n",
    "# Conecta o PostgreSQL e buscar dados de todas as tabelas\n",
    "def load_postgres_documents():\n",
    "    # conn = psycopg2.connect(**POSTGRES_CONFIG)\n",
    "    conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=\"5433\",\n",
    "    database=\"app_db\",\n",
    "    user=\"admin\",\n",
    "    password=\"admin\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT tablename FROM pg_tables WHERE schemaname = 'public'\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    documents_pg = []\n",
    "    for table in tables:\n",
    "        cursor.execute(f\"SELECT * FROM {table}\")\n",
    "        rows = cursor.fetchall()\n",
    "        colnames = [desc[0] for desc in cursor.description]\n",
    "        for row in rows:\n",
    "            content = \"\\n\".join(f\"{col}: {val}\" for col, val in zip(colnames, row))\n",
    "            documents_pg.append(Document(page_content=content, metadata={\"source\": \"postgresql\", \"doc_type\": table}))\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return documents_pg\n",
    "\n",
    "# Carregar arquivos .md de cada pasta\n",
    "def load_markdown_documents():\n",
    "    folders = glob.glob(\"../knowledge_base/*\")\n",
    "    text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "    documents = []\n",
    "\n",
    "    def add_metadata(doc, doc_type):\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        return doc\n",
    "\n",
    "    for folder in folders:\n",
    "        doc_type = os.path.basename(folder)\n",
    "        loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "        folder_docs = loader.load()\n",
    "        documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Dividir os documentos em chunks\n",
    "def split_documents(documents):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators=[\"\\n\\n\",\"\\n\",\".\",\" \", \"\"])\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "milvus_host = os.getenv(\"MILVUS_HOST\", \"milvus\")\n",
    "milvus_port = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "\n",
    "# Conectar ao Milvus\n",
    "def connect_to_milvus():\n",
    "    connections.connect(host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Inserir documentos vetoriais no Milvus\n",
    "def insert_into_milvus(chunks,collection_name=\"prediza_chunks\", host=\"localhost\",port=\"19530\",\n",
    "                       allow_append=False):\n",
    "    #embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"))\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
    "    \n",
    "    if utility.has_collection(collection_name):\n",
    "        print(f\"[INFO] A coleção '{collection_name}' já existe\")\n",
    "        if allow_append:\n",
    "            print(\"[INFO] Inserindo novos documentos na coleção existente...\")\n",
    "            vectorstore = Milvus(\n",
    "                embedding_function=embeddings,\n",
    "                collection_name=collection_name,\n",
    "                connection_args={\"host\": host, \"port\": port}\n",
    "            )\n",
    "            vectorstore.add_documents(chunks)\n",
    "\n",
    "        else:\n",
    "            print(\"[INFO] Recuperando a coleção existente sem modificá-la...\")\n",
    "            vectorstore = Milvus(embedding_function=embeddings, collection_name=collection_name, connection_args={\"host\": host, \"port\": port})\n",
    "    else:    \n",
    "        print(f\"[INFO] Criando a coleção '{collection_name}' e inserindo documentos...\")\n",
    "        vectorstore = Milvus.from_documents(chunks, embedding=embeddings, collection_name=collection_name, connection_args={\"host\": \"localhost\", \"port\": \"19530\"})#connection_args={\"host\": milvus_host, \"port\": milvus_port})\n",
    "        print(\"[INFO] Dados inseridos no Milvus com sucesso.\")\n",
    "    return vectorstore\n",
    "\n",
    "def chat(question, history):\n",
    "    print(\"Pergunta recebida:\", question)\n",
    "    # Invoca a cadeia de conversação com retorno de fontes\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "665abfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectado: True\n",
      "Coleções no Milvus:\n",
      "- prediza_chunks\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, utility, Collection\n",
    "\n",
    "# Conecte-se ao Milvus na porta 19530\n",
    "connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "status = connections.has_connection(\"default\")\n",
    "print(\"Conectado:\", status)\n",
    "\n",
    "collections = utility.list_collections()\n",
    "print(\"Coleções no Milvus:\")\n",
    "for collection in collections:\n",
    "    print(\"-\", collection)\n",
    "    bd = Collection(collection)\n",
    "    bd.load()\n",
    "    print(bd.num_entities)\n",
    "# utility.drop_collection(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fa81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility.drop_collection(\"prediza_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0375541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando pipeline RAG...\n",
      "[INFO] A coleção 'prediza_chunks' já existe\n",
      "[INFO] Recuperando a coleção existente sem modificá-la...\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando pipeline RAG...\")\n",
    "\n",
    "postgres_docs = load_postgres_documents()\n",
    "md_docs = load_markdown_documents()\n",
    "all_docs = postgres_docs + md_docs\n",
    "chunks = split_documents(all_docs)\n",
    "\n",
    "connect_to_milvus()\n",
    "vectorstore= insert_into_milvus(chunks,collection_name=\"prediza_chunks\", host=\"localhost\",port=\"19530\",\n",
    "                       allow_append=False) # manter allow_append como False (erro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263aa705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Configurando modelo e cadeia de conversação...\")\n",
    "\n",
    "# ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://ollama:11434\")\n",
    "\n",
    "\n",
    "# # Mensagens de sistema\n",
    "# system_message = \"Você é especialista em responder perguntas precisas sobre a empresa Prediza. Seja breve e preciso. Se não souber a resposta, diga. Não invente nada se não tiver recebido contexto relevante.Você deve usar apenas os documentos fornecidos para responder. Se não encontrar nada nos documentos, responda: Não sei. Isso não consta nos dados da Prediza.\"\n",
    "# idioma = \"Sempre responda no idioma Português, Brasil.\"\n",
    "\n",
    "# # Cria um prompt customizado com o system_message + idioma\n",
    "# system_template = system_message + \"\\n\" + idioma\n",
    "# system_msg_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# human_msg_prompt = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([system_msg_prompt, human_msg_prompt])\n",
    "\n",
    "\n",
    "# llm = ChatOllama(temperature=0.7, model=\"phi4-mini\", base_url=\"http://localhost:11434\")#, prompt=chat_prompt)\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,output_key=\"answer\")\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# #retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f56f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's try a simple question\n",
    "# # query = \"O que ou quem é Prediza?\"\n",
    "# # query = \"O que é NDVI?\"\n",
    "# query = \"Qual a formula da hipotenuza\"\n",
    "# result = conversation_chain.invoke({\"question\": query})\n",
    "# print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando modelo e cadeia de conversação...\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate what gets sent behind the scenes\n",
    "# from langchain_core.callbacks import StdOutCallbackHandler # callbacks=[StdOutCallbackHandler()]\n",
    "# print(\"Configurando modelo e cadeia de conversação...\")\n",
    "\n",
    "# ollama_base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://ollama:11434\")\n",
    "\n",
    "\n",
    "# # Mensagens de sistema\n",
    "# system_message = \"Você é especialista em responder perguntas precisas sobre a empresa Prediza. Seja breve e preciso. Se não souber a resposta, diga. Não invente nada se não tiver recebido contexto relevante.Você deve usar apenas os documentos fornecidos para responder. Se não encontrar nada nos documentos, responda: Não sei. Isso não consta nos dados da Prediza.\"\n",
    "# idioma = \"Sempre responda no idioma Português, Brasil.\"\n",
    "\n",
    "# # Cria um prompt customizado com o system_message + idioma\n",
    "# system_template = system_message + \"\\n\" + idioma\n",
    "# system_msg_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# human_msg_prompt = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "# chat_prompt = ChatPromptTemplate.from_messages([system_msg_prompt, human_msg_prompt])\n",
    "\n",
    "\n",
    "# llm = ChatOllama(temperature=0.7, model=\"phi4-mini\", base_url=\"http://localhost:11434\")#, prompt=chat_prompt)\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,output_key=\"answer\")\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# #retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory,return_source_documents=True,callbacks=[StdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a simple question\n",
    "# query = \"O que ou quem é Prediza?\"\n",
    "# query = \"O que é NDVI?\"\n",
    "# query = \"Qual a formula da hipotenuza\"\n",
    "# result = conversation_chain.invoke({\"question\": query})\n",
    "# print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4623a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(temperature=0.7, model=\"phi4-mini\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Prompt customizado traduzido e adaptado\n",
    "system_message = (\n",
    "    \"Use os seguintes trechos de contexto para responder à pergunta do usuário.\\n\"\n",
    "    \"Se você não souber a resposta, apenas diga que não sabe — não invente uma resposta.\\n\"\n",
    "    \"Não responda nada fora do contexto fornecido.\\n\"\n",
    "    'Se a informação não estiver nos documentos, diga: \"Não sei. Isso não consta nos dados da Prediza.\"'\n",
    ")\n",
    "\n",
    "# Prompt formatado\n",
    "system_msg_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_msg_prompt = HumanMessagePromptTemplate.from_template(\"Contexto:\\n{context}\\n\\nPergunta: {question}\")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg_prompt, human_msg_prompt])\n",
    "\n",
    "# Prompt para geração da próxima pergunta com base no histórico\n",
    "condense_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Dada a conversa anterior e a nova pergunta de acompanhamento, reformule a nova pergunta de forma independente.\n",
    "Histórico do chat: {chat_history} Pergunta de acompanhamento: {question} Pergunta reformulada: \"\"\")\n",
    "\n",
    "# Cadeias\n",
    "question_generator = LLMChain(llm=llm, prompt=condense_prompt)\n",
    "qa_chain = StuffDocumentsChain(llm_chain=LLMChain(llm=llm, prompt=chat_prompt), document_variable_name=\"context\")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True,output_key=\"answer\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Cadeia final com logs\n",
    "conversation_chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    question_generator= question_generator,\n",
    "    combine_docs_chain=qa_chain,\n",
    "    return_source_documents=True,\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0b0aa25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Não sei. Isso não consta nos dados da Prediza.\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simple question\n",
    "#query = \"O que ou quem é Prediza?\"\n",
    "# query = \"O que é NDVI?\"\n",
    "#query = \"Quais serviços da Prediza ?\"\n",
    "\n",
    "query = \"Crie um texto da historia dos 3 porquinhos\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline RAG finalizado. Iniciando Gradio...\")\n",
    "\n",
    "# Interface Gradio\n",
    "view = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    chatbot=gr.Chatbot(height=400, type=\"messages\"),\n",
    "    title=\"RAG Assistant - Prediza\",\n",
    "    theme=\"soft\",\n",
    "    description=\"Faça perguntas sobre insights e dados Prediza.\",\n",
    "    type=\"messages\",\n",
    ")\n",
    "view.launch(server_name=\"0.0.0.0\", server_port=7864, inbrowser=False)\n",
    "\n",
    "# # Gradio:\n",
    "# view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
